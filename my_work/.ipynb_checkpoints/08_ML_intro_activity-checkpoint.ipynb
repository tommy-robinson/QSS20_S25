{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning: Introduction and regularization \n",
    "## Binary classification with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "import pickle\n",
    "\n",
    "## nltk imports\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to process text\n",
    "def processtext(one_str, stop_list):\n",
    "    \n",
    "    ## remove stopwords\n",
    "    no_stop = [tok for tok in wordpunct_tokenize(one_str)\n",
    "              if tok not in stop_list]\n",
    "    \n",
    "    \n",
    "    processed_string = \" \".join([porter.stem(i.lower()) \n",
    "                        for i in no_stop if \n",
    "                        i.lower().isalpha() and len(i) >=3])\n",
    "    return(processed_string)\n",
    "\n",
    "## function to create dtm\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load labeled yelp data in `public_data` and run below code\n",
    "\n",
    "**Note**: make sure to change your path if you need to; if you're having trouble loading the `pkl`, try running on jupyter hub since it may be a python versioning issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trouble loading these data (kernel dies due to memory issues), try sampling down to 5000 or 1000 rows\n",
    "yelp = pd.read_pickle(\"../public_data/yelp_forML.pkl\") #.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess data to create dtm\n",
    "porter = PorterStemmer()\n",
    "list_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "yelp['process_text'] = [processtext(one_review, stop_list = list_stopwords) \n",
    "                        for one_review in yelp['raw_text']]\n",
    "\n",
    "yelp_dtm = create_dtm(yelp['process_text'], yelp[['metadata_label', 'metadata_rowid',\n",
    "                                                 'process_text', 'raw_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split into features, labels, and split into training/hold out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Split into X (features or id metadata) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_dtm[[col for col in yelp_dtm.columns if col not in ['metadata_label',\n",
    "                                                            'index']]].copy()\n",
    "y = yelp_dtm[['metadata_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking dimensionality\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert y.shape[1] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 using automatic function to create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using built-in function  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a great function and use it as much as you can.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 using more manual approach to create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### more manually: useful when we want more control\n",
    "### over the ids (eg clustering or time ordering)\n",
    "### or if we want to go back to matrix before preprocessing\n",
    "nrows_train = round(X.shape[0]*0.8)\n",
    "nrows_test = X.shape[0] - nrows_train\n",
    "random.seed(221)\n",
    "train_ids = random.sample(list(set(X['metadata_rowid'])), nrows_train)\n",
    "\n",
    "def my_split(X, y, \n",
    "             train_ids, \n",
    "             id_col):\n",
    "    \n",
    "    ## get test ids\n",
    "    test_ids = set(X[id_col]).difference(train_ids)\n",
    "    \n",
    "    ## split\n",
    "    X_train_man = X[X[id_col].isin(train_ids)].copy()\n",
    "    X_test_man = X[X[id_col].isin(test_ids)].copy()\n",
    "    y_train_man = y[y.index.isin(train_ids)].iloc[:, 0].to_numpy()\n",
    "    y_test_man = y[y.index.isin(test_ids)].iloc[:, 0].to_numpy()\n",
    "    \n",
    "    ## return\n",
    "    return(X_train_man, X_test_man, y_train_man, y_test_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_man, X_test_man, y_train_man, y_test_man = my_split(X, y,\n",
    "                                                            train_ids, \n",
    "                                                            id_col = 'metadata_rowid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimate models with hardcoded parameters: logistic regression with L1 regularization (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimate model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_feat = ['metadata_rowid', 'raw_text', 'process_text']\n",
    "logit_lasso = LogisticRegression(penalty = \"l1\",max_iter=100, \n",
    "             C = 0.01, solver='liblinear')\n",
    "logit_lasso.fit(X_train_man[[col for col in X_train.columns if col not in \n",
    "                   non_feat]], y_train_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generate predictions in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit_lasso.predict(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])\n",
    "y_predprob = logit_lasso.predict_proba(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the results \n",
    "y_pred[0:5]\n",
    "y_predprob[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clean up predictions and calculate error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make into a dataframe\n",
    "y_pred_df = pd.DataFrame({'y_pred_binary': y_pred,\n",
    "                         'y_pred_continuous': [one_prob[1] \n",
    "                                            for one_prob in y_predprob],\n",
    "                         'y_true': y_test_man})\n",
    "y_pred_df.sample(n = 10, random_state = 4484)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision as tp / tp+fp \n",
    "error_cond = [(y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 1),\n",
    "             (y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 0),\n",
    "              (y_pred_df['y_true'] == 0) & (y_pred_df['y_pred_binary'] == 0)]\n",
    "\n",
    "error_codeto = [\"TP\", \"FN\", \"TN\"]\n",
    "\n",
    "y_pred_df['error_cat'] = np.select(error_cond, error_codeto, default = \"FP\")\n",
    "y_error = y_pred_df.error_cat.value_counts().reset_index().copy()\n",
    "y_error.columns = ['cat', 'n']\n",
    "y_error\n",
    "\n",
    "### precision\n",
    "print(\"Precision is:-----------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FP\", 'n'].iloc[0])\n",
    "\n",
    "### recall\n",
    "print(\"Recall is:---------------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FN\", 'n'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpret the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top features\n",
    "las_coef = pd.DataFrame({'coef': logit_lasso.coef_[0],\n",
    "                         'feature_name': \n",
    "                        [col for col in X_train.columns if col not in non_feat]})\n",
    "las_coef.sort_values(by = 'coef', ascending = False)\n",
    "\n",
    "\n",
    "top_feat = las_coef.sort_values(by = 'coef', ascending = False)[0:10]\n",
    "top_feat_list = top_feat.feature_name.to_list()\n",
    "\n",
    "all_agg = [yelp_dtm.groupby(['metadata_label']).agg({one_feat: np.mean})\n",
    "for one_feat in top_feat_list]\n",
    "all_agg_df = pd.concat(all_agg, axis = 1)\n",
    "all_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare performance across hyperparameters for logistic regression\n",
    "\n",
    "Would our logit model make more accurate predictions if we fed it different hyperpameters? Which hyperparameters would be the best? Let's find out.\n",
    "\n",
    "1. Define a function that:\n",
    "- takes in a cost parameter (*C*, the inverse of regularization strength)\n",
    "- trains a logistic regression model with L1 regularization (Lasso) and otherwise has the same parameters as above\n",
    "- fits the model on the training data\n",
    "- makes predictions and returns them as a DataFrame\n",
    "\n",
    "2. Use the function to get predictions for the list of *C* parameters below, then bind them into one DataFrame.\n",
    "\n",
    "3. Finally, score the precision for each model (each iteration of *C*) and show which model scores the best.\n",
    "\n",
    "**Hint**: To compute precision score, you can use:\n",
    "```python\n",
    "precision_score(\n",
    "    one_df['y_true'], one_df['y_pred'],\n",
    "    zero_division = 0) # silences warning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided set of hyperparameters on which to train and then compare performance\n",
    "c_list = np.linspace(4, 0.0001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to evaluate precision of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Activity \n",
    "\n",
    "- Read the documentation here to initialize a ridge regression (l2 penalty)- you can use the same cost parameter (C) and number of iterations as in the lasso example above: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Fit the model on X_train_man, y_train_main \n",
    "- Generate binary and continuous predictions\n",
    "- Create a function that takes in a dataframe of binary predictions and true labels and manually calculates the $F_{1}$ score:\n",
    "\n",
    "$$F_{1} = 2 * \\dfrac{precision * recall}{precision + recall} = \\dfrac{TP}{TP + 0.5(FP + FN)}$$\n",
    "\n",
    "- Apply that function to calculate the F1 score for the decision tree and lasso (from above), and ridge regression (from the activity)\n",
    "- *Challenge exercise*: parametrize the model fitting with a function that takes in a classifier as an argument and returns coefficients or feature importances and certain eval metrics (eg precision, recall, and F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
